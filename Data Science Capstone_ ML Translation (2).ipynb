{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "# Only read this in once or it will return a NULL. Read in data from streaming_body_1.\nreadrawdata = streaming_body_1.read()"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "(240000, 3)\n"
                }
            ],
            "source": "# StringIO needed to convert bytes file to pandas dataframe.\nfrom io import StringIO\n\n# Convert bytes file to pandas dataframe.\ns=str(readrawdata,'utf-8')\ndata = StringIO(s) \n#First two hundred value in dataset.\ndf=pd.read_csv(data, sep=\"\\t\", header=None)\n# As discussed in \"Data exploration\", this is a pretty ideal chunk of training data, 240,000 rows long, to use to translate phrases from children's books.\ndf = df[120000:360000]\nprint(df.shape)\n\n"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": "# To save memory, delete unecessary variables from memory. \ndel s, data, readrawdata, endpoint_c9a37f3a569f4269ad9b640bc13792f3, client_c9a37f3a569f4269ad9b640bc13792f3, streaming_body_1"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "# Delete attribution column\ndf = df.drop(df.columns[[2]], axis=1)"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "# Add column names\ndf.columns = ['English', 'Russian']"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "/opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\n/opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\nCollecting tensorflow==2.6.0\n  Downloading tensorflow-2.6.0-cp37-cp37m-manylinux2010_x86_64.whl (458.3 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 458.3 MB 17 kB/s s eta 0:00:01\n\u001b[?25hRequirement already satisfied: wrapt~=1.12.1 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from tensorflow==2.6.0) (1.12.1)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from tensorflow==2.6.0) (0.2.0)\nCollecting h5py~=3.1.0\n  Downloading h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.0 MB 49.8 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: wheel~=0.35 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from tensorflow==2.6.0) (0.35.1)\nCollecting flatbuffers~=1.12.0\n  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\nRequirement already satisfied: six~=1.15.0 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from tensorflow==2.6.0) (1.15.0)\nRequirement already satisfied: numpy~=1.19.2 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from tensorflow==2.6.0) (1.19.2)\nCollecting clang~=5.0\n  Downloading clang-5.0.tar.gz (30 kB)\nCollecting keras~=2.6\n  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.3 MB 43.2 MB/s eta 0:00:01\n\u001b[?25hCollecting grpcio<2.0,>=1.37.0\n  Downloading grpcio-1.40.0-cp37-cp37m-manylinux2014_x86_64.whl (4.3 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.3 MB 40.6 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from tensorflow==2.6.0) (1.1.0)\nRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from tensorflow==2.6.0) (1.6.3)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from tensorflow==2.6.0) (3.11.2)\nRequirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from tensorflow==2.6.0) (3.7.4.2)\nCollecting tensorboard~=2.6\n  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.6 MB 41.2 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: absl-py~=0.10 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from tensorflow==2.6.0) (0.10.0)\nCollecting opt-einsum~=3.3.0\n  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 65 kB 6.6 MB/s  eta 0:00:01\n\u001b[?25hCollecting gast==0.4.0\n  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\nCollecting keras-preprocessing~=1.1.2\n  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 42 kB 2.5 MB/s  eta 0:00:01\n\u001b[?25hCollecting tensorflow-estimator~=2.6\n  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 462 kB 50.3 MB/s eta 0:00:01\n\u001b[?25hCollecting cached-property; python_version < \"3.8\"\n  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\nRequirement already satisfied: setuptools in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from protobuf>=3.9.2->tensorflow==2.6.0) (47.3.1.post20200622)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.6.0) (0.4.1)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.6.0) (3.1.1)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.6.0) (2.25.1)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.6.0) (1.6.0)\nCollecting tensorboard-data-server<0.7.0,>=0.6.0\n  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.9 MB 47.6 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.6.0) (1.23.0)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.6.0) (1.0.1)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.6.0) (1.3.0)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6.0) (2.8)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6.0) (2021.5.30)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6.0) (3.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6.0) (1.26.6)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow==2.6.0) (0.2.8)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow==2.6.0) (4.1.1)\nRequirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow==2.6.0) (4.6)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.6.0) (3.1.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow==2.6.0) (0.4.8)\nBuilding wheels for collected packages: clang\n  Building wheel for clang (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30705 sha256=4abc52a48a03044ed281392e1b34abed28ad5e29329a4c1f1515958f3e8e8d5c\n  Stored in directory: /tmp/wsuser/.cache/pip/wheels/98/91/04/971b4c587cf47ae952b108949b46926f426c02832d120a082a\nSuccessfully built clang\nInstalling collected packages: cached-property, h5py, flatbuffers, clang, keras, grpcio, tensorboard-data-server, tensorboard, opt-einsum, gast, keras-preprocessing, tensorflow-estimator, tensorflow\n  Attempting uninstall: h5py\n    Found existing installation: h5py 2.10.0\n    Uninstalling h5py-2.10.0:\n      Successfully uninstalled h5py-2.10.0\n  Attempting uninstall: flatbuffers\n    Found existing installation: flatbuffers 20210226132247\n    Uninstalling flatbuffers-20210226132247:\n      Successfully uninstalled flatbuffers-20210226132247\n  Attempting uninstall: grpcio\n    Found existing installation: grpcio 1.35.0\n    Uninstalling grpcio-1.35.0:\n      Successfully uninstalled grpcio-1.35.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.4.1\n    Uninstalling tensorboard-2.4.1:\n      Successfully uninstalled tensorboard-2.4.1\n  Attempting uninstall: opt-einsum\n    Found existing installation: opt-einsum 3.1.0\n    Uninstalling opt-einsum-3.1.0:\n      Successfully uninstalled opt-einsum-3.1.0\n  Attempting uninstall: gast\n    Found existing installation: gast 0.3.3\n    Uninstalling gast-0.3.3:\n      Successfully uninstalled gast-0.3.3\n  Attempting uninstall: keras-preprocessing\n    Found existing installation: Keras-Preprocessing 1.1.0\n    Uninstalling Keras-Preprocessing-1.1.0:\n      Successfully uninstalled Keras-Preprocessing-1.1.0\n  Attempting uninstall: tensorflow-estimator\n    Found existing installation: tensorflow-estimator 2.4.0\n    Uninstalling tensorflow-estimator-2.4.0:\n      Successfully uninstalled tensorflow-estimator-2.4.0\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.4.2\n    Uninstalling tensorflow-2.4.2:\n      Successfully uninstalled tensorflow-2.4.2\nSuccessfully installed cached-property-1.5.2 clang-5.0 flatbuffers-1.12 gast-0.4.0 grpcio-1.40.0 h5py-3.1.0 keras-2.6.0 keras-preprocessing-1.1.2 opt-einsum-3.3.0 tensorboard-2.6.0 tensorboard-data-server-0.6.1 tensorflow-2.6.0 tensorflow-estimator-2.6.0\n"
                }
            ],
            "source": "# Install Tensorflow, the framework I'll be using for this project. \n!pip install tensorflow==2.6.0\n"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "# Import necessary tensorflow packages and numpy libraries. \nimport tensorflow as tf\nimport numpy as np\nfrom numpy import array\nfrom numpy import argmax\nfrom numpy import random\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import load_model\nfrom nltk.translate.bleu_score import corpus_bleu"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "                        English                      Russian\n120000  no one owes me anything   \u043d\u0438\u043a\u0442\u043e \u043c\u043d\u0435 \u043d\u0438\u0447\u0435\u0433\u043e \u043d\u0435 \u0434\u043e\u043b\u0436\u0435\u043d\n120001  no one ran ahead of him  \u0432\u043f\u0435\u0440\u0435\u0434\u0438 \u043d\u0435\u0433\u043e \u043d\u0438\u043a\u0442\u043e \u043d\u0435 \u0431\u0435\u0436\u0430\u043b\n120002  no one slept that night      \u0432 \u0442\u0443 \u043d\u043e\u0447\u044c \u043d\u0438\u043a\u0442\u043e \u043d\u0435 \u0441\u043f\u0430\u043b\n120003  no one told me anything   \u043d\u0438\u043a\u0442\u043e \u043d\u0438\u0447\u0435\u0433\u043e \u043c\u043d\u0435 \u043d\u0435 \u0441\u043a\u0430\u0437\u0430\u043b\n120004  no one told me anything   \u043d\u0438\u043a\u0442\u043e \u043c\u043d\u0435 \u043d\u0438\u0447\u0435\u0433\u043e \u043d\u0435 \u0441\u043a\u0430\u0437\u0430\u043b\n"
                }
            ],
            "source": "import string\n# Name columns for clarity\n# Clean data by lowercasing and removing punctuation\n\ndf = df.apply(lambda x: x.astype(str).str.lower())\ndf = df.apply(lambda x: x.astype(str).str.replace(r'[^\\w\\s]+', ''))\n\nprint(df.head())"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": "# Functions. The grand purpose is to convert data to categorical, and output data to one-hot encoding. \nfrom tensorflow.keras.utils import to_categorical\n\n# Create a tokenizer, which assigns each unique word to a unique integer. This is necessary for categorical encoding. \ndef create_tokenizer(lines):\n\ttokenizer = Tokenizer()\n\ttokenizer.fit_on_texts(lines)\n\treturn tokenizer\n\n''' Finds the longest sentence in the translation set. This is necessary for creating tokenizer arrays, because each tokenized row\nwill have length max_length, and will be padded with zeros.'''\ndef max_length(lines):\n\treturn max(len(line.split()) for line in lines)\n\n'''Categorically encode using max length. For example, the string \"John went to the park.\" will be encoded as [123 3 5 2 435 0 0 0...]. \nThe length will be max_length, and each integer in the list corresponds to a word in the sentence (as encoded by the tokenizer).'''\ndef encode_sequences(tokenizer, length, lines):\n\t# integer encode sequences\n\tX = tokenizer.texts_to_sequences(lines)\n\t# pad sequences with 0 values\n\tX = pad_sequences(X, maxlen=length, padding='post')\n\treturn X\n\n'''One-hot encoding. This will create a sparse matrix for every row in the output set, of length max_length. Each list in the matrix will have length \nvocab_size, and a 1 will be at the index corresponding to the word's encoded token.'''\ndef encode_output(sequences, vocab_size):\n\tylist = list()\n\tfor sequence in sequences:\n\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n\t\tylist.append(encoded)\n\ty = array(ylist)\n\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n\treturn y"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": "# Create the tokenizer, calculate vocab case, and find max length of an English sentence in the set. All for future encoding.\neng_tokenizer = create_tokenizer(df[\"English\"])\neng_vocab_size = len(eng_tokenizer.word_index) + 1\neng_length = max_length(df['English'])"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": "# Create the tokenizer, calculate vocab case, and find max length of an Russian sentence in the set. All for future encoding.\nrus_tokenizer = create_tokenizer(df['Russian'])\nrus_vocab_size = len(rus_tokenizer.word_index) + 1\nrus_length = max_length(df['Russian'])"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "(240000,)\n14793\n"
                }
            ],
            "source": "# Uses the tokenizer to output a word with input of its index.\ndef word_for_id(integer, tokenizer):\n\tfor word, index in tokenizer.word_index.items():\n\t\tif index == integer:\n\t\t\treturn word\n\treturn None"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": "# First, use train_test_split to establish target and source datasets for training and testing. \nfrom sklearn.model_selection import train_test_split\n\n# Training set will have 192k values. Test set will have 48k.\nx_train, x_test, y_train, y_test =  train_test_split(df['Russian'], df['English'], test_size=0.2)\n\n''' Encode training datasets. I'm not one-hot-encoding the output because of the huge amount of memory required. I will be one-hot encoding the output\nin chunks as I feed it into the model using a generator. '''\ntrainX = encode_sequences(rus_tokenizer, rus_length, x_train)\ntrainY = encode_sequences(eng_tokenizer, eng_length, y_train)\n# 192,000 training cases. 58,000 test cases."
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [],
            "source": "# prepare validation data\ntestX = encode_sequences(rus_tokenizer, rus_length, x_test[0:10000])\ntestY = encode_sequences(eng_tokenizer, eng_length, y_test[0:10000])\ntestY = encode_output(testY, eng_vocab_size)\n"
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": "# Import all of the Tensorflow packages I'll need for my model.\nimport tensorflow.keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Bidirectional\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import RepeatVector\nfrom tensorflow.keras.layers import TimeDistributed\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.callbacks import ModelCheckpoint"
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": "# Define my model. \n'''First, I use an Embedding layer, which creates a vector space in which words with similar meanings are mapped more closely to each other. \nA long short-term memory layer is a type of recurrent neural network that has feedback as well as feed forward connections. Long story short, \nthe difference between a bidirectional LSTM and a conventional LSTM (unidirectional by default), is that a bidirectional LSTM runs inputs through\nin two orders, enabling it to preserve both past and future data. The RepeatVector layer essentially adds another dimension to your model of dimension\nbatch_size. The TimeDistributed layer applies a layer for a specified slice of unit time. It wraps up the model by forming a vector of probabilities. \nThe model is a sequential model with two Dropouts to prevent overfitting. I initially tried an LSTM, but research indicated that Bidirectional Long\nShort-Term Models are usually preferable to LSTMs in Natural Language Processing contexts. \n'''\ndef define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n\tmodel = Sequential()\n\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n\tmodel.add(Bidirectional(LSTM(n_units)))\n\tmodel.add(Dropout(0.2))\n\tmodel.add(RepeatVector(tar_timesteps))\n\tmodel.add(LSTM(n_units, return_sequences=True))\n\tmodel.add(Dropout(0.2))\n\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n\treturn model"
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": "# I found that an n-gram value of 512 not only slowed down the fitting, but increased overfitting. \n# adam tends to be the preferred optimizer for NLP processes. \nmodel = define_model(rus_vocab_size, eng_vocab_size, rus_length, eng_length, 256)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy')"
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 18, 256)           11665152  \n_________________________________________________________________\nbidirectional (Bidirectional (None, 512)               1050624   \n_________________________________________________________________\ndropout (Dropout)            (None, 512)               0         \n_________________________________________________________________\nrepeat_vector (RepeatVector) (None, 17, 512)           0         \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 17, 256)           787456    \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 17, 256)           0         \n_________________________________________________________________\ntime_distributed (TimeDistri (None, 17, 14793)         3801801   \n=================================================================\nTotal params: 17,305,033\nTrainable params: 17,305,033\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
                }
            ],
            "source": "# Below is a nice little summary of the model.\nprint(model.summary())"
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "/opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\n/opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\nCollecting pydot\n  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\nRequirement already satisfied: pyparsing>=2.1.4 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from pydot) (2.4.7)\nInstalling collected packages: pydot\nSuccessfully installed pydot-1.4.2\n/opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\n/opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\nRequirement already satisfied: graphviz in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (0.16)\n"
                }
            ],
            "source": "# Install the two things that will allow me to plot the model. \n!pip install pydot\n!pip install graphviz"
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
                }
            ],
            "source": "# Plot the model.\nimport pydot\nimport graphviz\nplot_model(model, to_file='model.png', show_shapes=True)\n"
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [],
            "source": "'''Evaluate the model using BLEU (bilingual evaluation understudy). In order to view the model outputs, we need a function for model predictions\nto compare it to the test data. '''\ndef predict_sequence(model, tokenizer, source):\n\tprediction = model.predict(source, verbose=0)[0]\n\tintegers = [argmax(vector) for vector in prediction]\n\ttarget = list()\n\tfor i in integers:\n\t\tword = word_for_id(i, tokenizer)\n\t\tif word is None:\n\t\t\tbreak\n\t\ttarget.append(word)\n\treturn ' '.join(target)\n\ndef evaluate_model(model, tokenizer, sources):\n\tactual, predicted = list(), list()\n\tfor i, source in enumerate(sources):\n\t\t# translate encoded source text\n\t\tsource = source.reshape((1, source.shape[0]))\n\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n\t\traw_target, raw_src = y_test.iloc[i], x_test.iloc[i]\n\t\tif i % 1000 == 0:\n\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n\t\tactual.append([raw_target.split()])\n\t\tpredicted.append(translation.split())\n\t# calculate BLEU score\n\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n"
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": "# data generator, intended to be used in a call to model.fit_generator().\n# The data generator one-hot encodes the output data, then feeds it into the model.\n# The output data will be in tensor form. If I didn't use a generator, my data would be of size 192,000 x ~11,000 x 18 = ~38 billion. \n# You need a lot more memory than I have at my disposal to train a model with a sparse tensor of size 38 billion. \n\ndef data_generator(source, target):\n    # loop for ever over images\n    while 1:\n        for x in range(0, 3000):\n            target_seg = encode_output(target[x*64:(x+1)*64], eng_vocab_size)\n            source_seg = source[x*64:(x+1)*64]\n            yield source_seg, target_seg"
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": "# Define the generator variable for use in model.fit\ngenerator = data_generator(trainX, trainY)"
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/20\n3000/3000 - 1053s - loss: 2.5477 - val_loss: 2.1381\n\nEpoch 00001: val_loss improved from inf to 2.13810, saving model to model.h5\nEpoch 2/20\n3000/3000 - 1033s - loss: 1.9820 - val_loss: 1.6826\n\nEpoch 00002: val_loss improved from 2.13810 to 1.68255, saving model to model.h5\nEpoch 3/20\n3000/3000 - 1040s - loss: 1.5597 - val_loss: 1.3488\n\nEpoch 00003: val_loss improved from 1.68255 to 1.34883, saving model to model.h5\nEpoch 4/20\n3000/3000 - 1044s - loss: 1.2800 - val_loss: 1.1721\n\nEpoch 00004: val_loss improved from 1.34883 to 1.17215, saving model to model.h5\nEpoch 5/20\n3000/3000 - 1040s - loss: 1.1029 - val_loss: 1.0753\n\nEpoch 00005: val_loss improved from 1.17215 to 1.07532, saving model to model.h5\nEpoch 6/20\n3000/3000 - 1033s - loss: 0.9819 - val_loss: 1.0192\n\nEpoch 00006: val_loss improved from 1.07532 to 1.01916, saving model to model.h5\nEpoch 7/20\n3000/3000 - 1034s - loss: 0.8937 - val_loss: 0.9862\n\nEpoch 00007: val_loss improved from 1.01916 to 0.98624, saving model to model.h5\nEpoch 8/20\n3000/3000 - 1039s - loss: 0.8238 - val_loss: 0.9654\n\nEpoch 00008: val_loss improved from 0.98624 to 0.96541, saving model to model.h5\nEpoch 9/20\n3000/3000 - 1046s - loss: 0.7686 - val_loss: 0.9514\n\nEpoch 00009: val_loss improved from 0.96541 to 0.95136, saving model to model.h5\nEpoch 10/20\n3000/3000 - 1043s - loss: 0.7236 - val_loss: 0.9438\n\nEpoch 00010: val_loss improved from 0.95136 to 0.94381, saving model to model.h5\nEpoch 11/20\n3000/3000 - 1046s - loss: 0.6854 - val_loss: 0.9351\n\nEpoch 00011: val_loss improved from 0.94381 to 0.93515, saving model to model.h5\nEpoch 12/20\n3000/3000 - 1046s - loss: 0.6529 - val_loss: 0.9344\n\nEpoch 00012: val_loss improved from 0.93515 to 0.93437, saving model to model.h5\nEpoch 13/20\n3000/3000 - 1048s - loss: 0.6240 - val_loss: 0.9318\n\nEpoch 00013: val_loss improved from 0.93437 to 0.93180, saving model to model.h5\nEpoch 14/20\n3000/3000 - 1042s - loss: 0.5986 - val_loss: 0.9363\n\nEpoch 00014: val_loss did not improve from 0.93180\nEpoch 15/20\n3000/3000 - 1043s - loss: 0.5765 - val_loss: 0.9376\n\nEpoch 00015: val_loss did not improve from 0.93180\nEpoch 16/20\n3000/3000 - 1039s - loss: 0.5564 - val_loss: 0.9367\n\nEpoch 00016: val_loss did not improve from 0.93180\nEpoch 17/20\n3000/3000 - 1045s - loss: 0.5387 - val_loss: 0.9490\n\nEpoch 00017: val_loss did not improve from 0.93180\nEpoch 18/20\n"
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-34-0047fe01fcb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'model.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[0;32m/opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
                        "\u001b[0;32m/opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": "# Define name of model to be stored in Object Storage. A checkpoint will be called after every epoch to calculate the loss using test data. \n# The model with the lowest calculated loss on the test data will be saved to Object storage. \n# The validation data is of size 2,000, because with my 16 GB of RAM, this seems to be roughly the maximum number that doesn't result in kernel restarts. \nfilename = 'model.h5'\ncheckpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nmodel.fit(generator, epochs=20, steps_per_epoch=3000, verbose=2, batch_size=64, callbacks=[checkpoint], validation_data=(testX, testY))\n"
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": "# Load that model from Object Storage. \nmodel = load_model('model.h5')\n"
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "src=[\u0432 \u0432\u0435\u0442\u0440\u043e\u0432\u043e\u043c \u0441\u0442\u0435\u043a\u043b\u0435 \u0431\u044b\u043b\u043e \u043f\u0443\u043b\u0435\u0432\u043e\u0435 \u043e\u0442\u0432\u0435\u0440\u0441\u0442\u0438\u0435], target=[there was a bullet hole in the windshield], predicted=[a was was a in in the window]\nsrc=[\u0434\u0435\u0440\u0435\u0432\u043d\u044f \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u0441\u044f \u0437\u0430 \u0442\u0435\u043c\u0438 \u0434\u0435\u0440\u0435\u0432\u044c\u044f\u043c], target=[the village is beyond those trees], predicted=[the is is the these trees are]\nsrc=[\u044f \u0437\u043d\u0430\u044e \u0447\u0442\u043e \u0443 \u0442\u0435\u0431\u044f \u0435\u0441\u0442\u044c \u0434\u043e\u043c\u0430\u0448\u043d\u044f\u044f \u0440\u0430\u0431\u043e\u0442\u0430 \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u0441\u043b\u0435\u0434\u0443\u0435\u0442 \u0432\u044b\u043f\u043e\u043b\u043d\u0438\u0442\u044c], target=[i know you have homework to do], predicted=[i know you have have homework do do]\nsrc=[\u0432 \u043f\u0440\u043e\u0448\u043b\u044b\u0435 \u0432\u044b\u0445\u043e\u0434\u043d\u044b\u0435 \u0442\u043e\u043c \u0431\u044b\u043b \u0441 \u0441\u0435\u043c\u044c\u0451\u0439], target=[tom was with his family last weekend], predicted=[tom was with last last last]\nsrc=[\u043c\u043d\u0435 \u043d\u0430\u0434\u043e \u0447\u0442\u043e\u0431\u044b \u0442\u044b \u043f\u043e\u0433\u043e\u0432\u043e\u0440\u0438\u043b \u0441 \u0442\u043e\u043c\u043e\u043c], target=[i need you to talk to tom], predicted=[i need you to talk to tom]\nsrc=[\u044f \u043f\u043e\u043f\u0430\u043b\u0430 \u043f\u043e\u0434 \u043b\u0438\u0432\u0435\u043d\u044c], target=[i was caught in a shower], predicted=[i was caught in a rainstorm]\nsrc=[\u043c\u044b \u0445\u043e\u0434\u0438\u043b\u0438 \u0432\u043c\u0435\u0441\u0442\u0435 \u0432 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0443], target=[we went to the library together], predicted=[we went to the library library]\nsrc=[\u044f \u0437\u043d\u0430\u043b \u0447\u0442\u043e \u043d\u0430\u043c \u043d\u0430\u0434\u043e \u0431\u044b\u043b\u043e \u0443\u0439\u0442\u0438 \u0440\u0430\u043d\u044c\u0448\u0435], target=[i knew we shouldve left earlier], predicted=[i knew we shouldve to earlier earlier]\nsrc=[\u043d\u0430 \u0441\u043e\u043b\u043d\u0446\u0435 \u0431\u044b\u043b\u043e \u0436\u0430\u0440\u043a\u043e \u0438 \u043e\u043d\u0438 \u0443\u0441\u0442\u0430\u043b\u0438], target=[the sun was hot and they were tired], predicted=[it was very hot and they was]\nsrc=[\u044f \u0435\u0434\u0438\u043d\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0439 \u0441 \u0432\u0430\u043c\u0438 \u043d\u0435 \u0441\u043e\u0433\u043b\u0430\u0441\u0435\u043d], target=[am i the only one that doesnt agree with you], predicted=[im am the only one who dont agree with you]\nBLEU-1: 0.661139\nBLEU-2: 0.541894\nBLEU-3: 0.487327\nBLEU-4: 0.380674\n"
                }
            ],
            "source": "# Evaluate the model using BLEU. First it spits out some sample predictions. As you can see, the model does pretty well. \n# The upper bound on the BLEU score is > .6, which is considered to be \"better than most translators.\"\nevaluate_model(model, eng_tokenizer, testX)"
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "src=[\u043e\u043d\u0430 \u043f\u0440\u043e\u0431\u0443\u0434\u0435\u0442 \u0432 \u043d\u044c\u044e\u0439\u043e\u0440\u043a\u0435 \u0434\u0432\u0435 \u043d\u0435\u0434\u0435\u043b\u0438], target=[she will be in new york for two weeks], predicted=[she will be in new york for a weeks]\nsrc=[\u0442\u043e\u043c \u0437\u0430\u043a\u043e\u043d\u0447\u0438\u043b \u0440\u0430\u0431\u043e\u0442\u0443 \u0437\u0430 \u0442\u0440\u0438 \u0447\u0430\u0441\u0430], target=[tom finished the job in three hours], predicted=[tom finished the work for three]\nsrc=[\u0442\u043e\u043c \u0437\u043d\u0430\u0435\u0442 \u0447\u0442\u043e \u043c\u044d\u0440\u0438 \u043d\u0435 \u0433\u043e\u0432\u043e\u0440\u0438\u0442 \u043f\u043e\u0444\u0440\u0430\u043d\u0446\u0443\u0437\u0441\u043a\u0438], target=[tom knows that mary cant speak french], predicted=[tom knows mary mary cant speak french]\nsrc=[\u044f \u043b\u0443\u0447\u0448\u0435 \u043d\u0435 \u0431\u0443\u0434\u0443 \u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u044d\u0442\u043e\u0442 \u0444\u0438\u043b\u044c\u043c], target=[id rather not see that movie], predicted=[id rather be see that movie movie]\nsrc=[\u043e\u043d \u0441\u0442\u0430\u043b \u0432\u0435\u043b\u0438\u043a\u0438\u043c \u043c\u0443\u0437\u044b\u043a\u0430\u043d\u0442\u043e\u043c], target=[he became a great musician], predicted=[he became a great musician]\nsrc=[\u043a\u0443\u0434\u0430 \u0432\u044b \u0435\u0437\u0434\u0438\u043b\u0438 \u0432 \u043f\u043e\u043d\u0435\u0434\u0435\u043b\u044c\u043d\u0438\u043a], target=[where did you go on monday], predicted=[where did you go on monday]\nsrc=[\u0442\u043e\u043c \u043b\u044e\u0431\u0438\u0442 \u0438\u0433\u0440\u0430\u0442\u044c \u0432 \u0431\u0430\u0441\u043a\u0435\u0442\u0431\u043e\u043b], target=[tom likes to play basketball], predicted=[tom likes playing play basketball]\nsrc=[\u043e\u043d \u043a\u0430\u0437\u0430\u043b\u043e\u0441\u044c \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u0442 \u0443\u0434\u043e\u0432\u043e\u043b\u044c\u0441\u0442\u0432\u0438\u0435 \u043e\u0442 \u0441\u0432\u043e\u0435\u0439 \u0436\u0438\u0437\u043d\u0438 \u0438 \u0441\u0432\u043e\u0435\u0439 \u0440\u0430\u0431\u043e\u0442\u044b], target=[he seemed to enjoy his life and his work], predicted=[he seemed to be his his his work his]\nsrc=[\u0442\u0435\u043f\u0435\u0440\u044c \u044f \u043f\u043e\u043d\u0438\u043c\u0430\u044e \u043c\u043d\u043e\u0433\u043e\u0435], target=[now i understand many things], predicted=[now i understand of things]\nsrc=[\u0437\u0430\u043a\u043e\u043d\u0447\u0438\u0432 \u0441\u0432\u043e\u044e \u0440\u0430\u0431\u043e\u0442\u0443 \u043e\u043d \u043f\u043e\u0448\u0451\u043b \u0434\u043e\u043c\u043e\u0439], target=[as soon as he finished his work he went home], predicted=[after he work job job he he went home home]\nBLEU-1: 0.766168\nBLEU-2: 0.669301\nBLEU-3: 0.620202\nBLEU-4: 0.522149\n"
                }
            ],
            "source": "evaluate_model(model, eng_tokenizer, trainX[0:10000])"
        },
        {
            "cell_type": "code",
            "execution_count": 69,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Phrases</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\u041e\u0434\u0435\u044f\u043b\u043e \u0423\u0431\u0435\u0436\u0430\u043b\u043e</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\u0423\u043b\u0435\u0442\u0435\u043b\u0430 \u043f\u0440\u043e\u0441\u0442\u044b\u043d\u044f</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\u0418 \u043f\u043e\u0434\u0443\u0448\u043a\u0430</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\u041a\u0430\u043a \u043b\u044f\u0433\u0443\u0448\u043a\u0430</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\u0423\u0441\u043a\u0430\u043a\u0430\u043b\u0430 \u043e\u0442 \u043c\u0435\u043d\u044f</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "             Phrases\n0     \u041e\u0434\u0435\u044f\u043b\u043e \u0423\u0431\u0435\u0436\u0430\u043b\u043e\n1  \u00a0\u0423\u043b\u0435\u0442\u0435\u043b\u0430 \u043f\u0440\u043e\u0441\u0442\u044b\u043d\u044f\n2         \u00a0\u0418 \u043f\u043e\u0434\u0443\u0448\u043a\u0430\n3       \u00a0\u041a\u0430\u043a \u043b\u044f\u0433\u0443\u0448\u043a\u0430\n4  \u00a0\u0423\u0441\u043a\u0430\u043a\u0430\u043b\u0430 \u043e\u0442 \u043c\u0435\u043d\u044f"
                    },
                    "execution_count": 69,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Phrases</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>745</th>\n      <td>\u0438 \u0441\u043d\u043e\u0432\u0430 \u043c\u0435\u0434\u0432\u0435\u0434\u044c</td>\n    </tr>\n    <tr>\n      <th>746</th>\n      <td>\u0441\u043f\u0430\u0441\u0438\u0442\u0435 \u043c\u043e\u0440\u0436\u0430</td>\n    </tr>\n    <tr>\n      <th>747</th>\n      <td>\u0432\u0447\u0435\u0440\u0430 \u043f\u0440\u043e\u0433\u043b\u043e\u0442\u0438\u043b \u043e\u043d \u043c\u043e\u0440\u0441\u043a\u043e\u0433\u043e \u0435\u0436\u0430</td>\n    </tr>\n    <tr>\n      <th>748</th>\n      <td>\u0438 \u0442\u0430\u043a\u0430\u044f \u0434\u0440\u0435\u0431\u0435\u0434\u0435\u043d\u044c \u0446\u0435\u043b\u044b\u0439 \u0434\u0435\u043d\u044c</td>\n    </tr>\n    <tr>\n      <th>749</th>\n      <td>\u0434\u0438\u043d\u044c\u0434\u0438\u043b\u0435\u043d\u044c</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "                             Phrases\n745                  \u0438 \u0441\u043d\u043e\u0432\u0430 \u043c\u0435\u0434\u0432\u0435\u0434\u044c\n746                    \u0441\u043f\u0430\u0441\u0438\u0442\u0435 \u043c\u043e\u0440\u0436\u0430\n747  \u0432\u0447\u0435\u0440\u0430 \u043f\u0440\u043e\u0433\u043b\u043e\u0442\u0438\u043b \u043e\u043d \u043c\u043e\u0440\u0441\u043a\u043e\u0433\u043e \u0435\u0436\u0430\n748     \u0438 \u0442\u0430\u043a\u0430\u044f \u0434\u0440\u0435\u0431\u0435\u0434\u0435\u043d\u044c \u0446\u0435\u043b\u044b\u0439 \u0434\u0435\u043d\u044c\n749                       \u0434\u0438\u043d\u044c\u0434\u0438\u043b\u0435\u043d\u044c"
                    },
                    "execution_count": 70,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# To lower.\nnp_df['Phrases'] = np_df['Phrases'].str.lower()\n# Get rid of numbers.\nnp_df['Phrases'] = np_df['Phrases'].str.replace('\\d+', '')\n# Get rid of non-Cyrillic characters. \nnp_df['Phrases'] = np_df['Phrases'].str.replace(r'[^\\w\\s]+', '')\n# Get rid of leading and trailing whitespace. \nnp_df['Phrases'] = np_df['Phrases'].str.strip()\n# Get rid of duplicate entries. \nnp_df = np_df.drop_duplicates(subset=None, keep='first', inplace=False)\n# Reset index. \nnp_df = np_df.reset_index()\nnp_df = np_df.drop(columns = ['index'])\n# This dataset has 799 entries, the other one has 1049. Let's cut them both to 750, so they're both the same size.\nnp_df = np_df.head(750)\nnp_df.tail()"
        },
        {
            "cell_type": "code",
            "execution_count": 67,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Phrases</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\u042d\u0442\u043e \u043f\u0435\u0440\u0432\u0430\u044f \u0441\u043e\u0432\u0435\u0442\u0441\u043a\u0430\u044f \u0444\u0430\u0431\u0440\u0438\u043a\u0430 \u0447\u0430\u044f</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\u041e\u043d\u0430 \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u0441\u044f \u0432 \u0441\u043e\u0432\u0445\u043e\u0437\u0435 \u201e\u0427\u0430\u043a\u0432\u0430\"</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\u0427\u0430\u043a\u0432\u0430</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\u0441\u0442\u0430\u043d\u0446\u0438\u044f \u0437\u0430\u043a\u0430\u0432\u043a\u0430\u0437\u0441\u043a\u043e\u0439 \u0436\u0435\u043b\u0435\u0437\u043d\u043e\u0439 \u0434\u043e\u0440\u043e\u0433\u0438</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\u041c\u043e\u043b\u043e\u0434\u044b\u0435 \u043b\u0438\u0441\u0442\u044c\u044f \u0447\u0430\u044f</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "                                Phrases\n0      \u042d\u0442\u043e \u043f\u0435\u0440\u0432\u0430\u044f \u0441\u043e\u0432\u0435\u0442\u0441\u043a\u0430\u044f \u0444\u0430\u0431\u0440\u0438\u043a\u0430 \u0447\u0430\u044f\n1       \u041e\u043d\u0430 \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u0441\u044f \u0432 \u0441\u043e\u0432\u0445\u043e\u0437\u0435 \u201e\u0427\u0430\u043a\u0432\u0430\"\n2                                 \u0427\u0430\u043a\u0432\u0430\n3  \u0441\u0442\u0430\u043d\u0446\u0438\u044f \u0437\u0430\u043a\u0430\u0432\u043a\u0430\u0437\u0441\u043a\u043e\u0439 \u0436\u0435\u043b\u0435\u0437\u043d\u043e\u0439 \u0434\u043e\u0440\u043e\u0433\u0438\n4                    \u041c\u043e\u043b\u043e\u0434\u044b\u0435 \u043b\u0438\u0441\u0442\u044c\u044f \u0447\u0430\u044f"
                    },
                    "execution_count": 67,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "#Load propaganda dataset. \nbody = client_c29a7a805d8d4196b70668aaf704d0e0.get_object(Bucket='default-donotdelete-pr-f13vke2upogt8x',Key='propaganda.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\np_df = pd.read_csv(body)\np_df.head()\n"
        },
        {
            "cell_type": "code",
            "execution_count": 85,
            "metadata": {},
            "outputs": [],
            "source": "#Refer to above.\np_df['Phrases'] = p_df['Phrases'].str.lower()\np_df['Phrases'] = p_df['Phrases'].str.replace('\\d+', '')\np_df['Phrases'] = p_df['Phrases'].str.replace(r'[^\\w\\s]+', '')\np_df['Phrases'] = p_df['Phrases'].str.strip()\np_df = p_df.drop_duplicates(subset=None, keep='first', inplace=False)\np_df = p_df.reset_index()\np_df = p_df.drop(columns = ['index'])\np_df = p_df.head(750)"
        },
        {
            "cell_type": "code",
            "execution_count": 91,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "bothers oral oral steep steep steep steep steep steep steep flush flush flush flush flush flush flush\n"
                }
            ],
            "source": "source = encode_sequences(rus_tokenizer, rus_length, p_df.iloc[511])\n#source = source.reshape((1, source.shape[0]))\ntranslation = predict_sequence(model, eng_tokenizer, source)\nprint(translation)"
        },
        {
            "cell_type": "code",
            "execution_count": 89,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Phrases    \u043d\u0435 \u0432\u044b\u043f\u0443\u0441\u043a\u0430\u0439\u0442\u0435 \u0432\u0438\u043d\u0442\u043e\u0432\u043e\u043a \u0438\u0437 \u0441\u0432\u043e\u0438\u0445 \u0440\u0443\u043a\nName: 510, dtype: object\n"
                }
            ],
            "source": "print(p_df.iloc[510])\n"
        },
        {
            "cell_type": "code",
            "execution_count": 71,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /home/wsuser/nltk_data...\n"
                },
                {
                    "data": {
                        "text/plain": "True"
                    },
                    "execution_count": 71,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "import nltk\nnltk.download('vader_lexicon')"
        },
        {
            "cell_type": "code",
            "execution_count": 72,
            "metadata": {},
            "outputs": [],
            "source": "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n"
        },
        {
            "cell_type": "code",
            "execution_count": 73,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "[{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0, 'headline': '\u043f\u0440\u0430\u0437\u0434\u043d\u043e\u0432\u0430\u043d\u0438\u0435 \u0441\u0442\u043e\u043b\u0435\u0442\u0438\u044f \u043c\u044e \u043b\u0435\u0440\u043c\u043e\u043d\u0442\u043e\u0432\u0430 \u043e\u0442\u043b\u043e\u0436\u0435\u043d\u043e\\t'}, {'neg': 0.0, 'neu': 0.513, 'pos': 0.487, 'compound': 0.2263, 'headline': 'Soldier surprises wife.'}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0, 'headline': 'Gas prices on the rise.'}, {'neg': 0.524, 'neu': 0.476, 'pos': 0.0, 'compound': -0.6597, 'headline': 'Jews murdered in the streets.'}, {'neg': 0.518, 'neu': 0.482, 'pos': 0.0, 'compound': -0.6486, 'headline': 'Osama Bin Laden is dead.'}]\n"
                }
            ],
            "source": "# testing\nsia = SIA()\nresults = []\nheadlines = [\"\u043f\u0440\u0430\u0437\u0434\u043d\u043e\u0432\u0430\u043d\u0438\u0435 \u0441\u0442\u043e\u043b\u0435\u0442\u0438\u044f \u043c\u044e \u043b\u0435\u0440\u043c\u043e\u043d\u0442\u043e\u0432\u0430 \u043e\u0442\u043b\u043e\u0436\u0435\u043d\u043e\t\", \"Soldier surprises wife.\", \"Gas prices on the rise.\", \"Jews murdered in the streets.\", \"Osama Bin Laden is dead.\"]\nfor line in headlines:\n    pol_score = sia.polarity_scores(line)\n    pol_score['headline'] = line\n    results.append(pol_score)\n\nprint(results)\n"
        },
        {
            "cell_type": "code",
            "execution_count": 137,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": " \u0440\u0443\u0441\u0441\u043a\u0438\u0435 \u0432\u043e\u0439\u0441\u043a\u0430 \u0432\u0441\u0442\u0443\u043f\u0438\u043b\u0438 \u0432\u00a0\u043f\u0440\u0435\u0434\u0435\u043b\u044b \u0432\u0435\u043d\u0433\u0440\u0438\u0438  \n"
                }
            ],
            "source": "print(news_df[\"title\"][0])\n#source = trainX[0].reshape((1, trainX[0].shape[0]))\n#print(source)\n#print(news_df[\"title\"].iloc[0])\n\n#print(news_df[\"title\"][0])\n#source = encode_sequences(eng_tokenizer, eng_length, news_df[\"title\"][0])\n#X = tokenizer.texts_to_sequences(lines)\n# pad sequences with 0 values\n#X = pad_sequences(X, maxlen=length, padding='post')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}